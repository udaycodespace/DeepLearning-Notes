# 🧠 Deep Learning (DLE) – VII Semester Survival Kit  

> *Because why just suffer in class when you can suffer here too?*  

---

### 📚 Course Info (aka: What we signed up for)  
- **Semester:** VII  
- **Branches:** CSE, CST, CSBS, CSE(AIML), CSE(DS) (basically everyone gets dragged in)  
- **Course Code:** CS407  
- **Category:** PEC-IV  
- **Credits:** 3 (but feels like 30)  
- **Scheme:** 2020 (and hopefully future-proof 🤞)  
- **Marks Split:**  
  - Internal: 40 (a.k.a. “write anything in exam, you’ll still get some marks”)  
  - External: 60 (a.k.a. “pray to all gods”)  
  - Total: 100  

---

### 🎯 Course Outcomes (COs) – Fancy way of saying "what they expect us to magically know"  
- **CO1:** Understand concepts of Deep Learning and ANN.  
- **CO2:** Pretend to summarize Deep Neural Nets.  
- **CO3:** Understand CNN operations (basically filters & magic).  
- **CO4:** Memorize CNN Architectures (LeNet, AlexNet, ResNet – Pokémon evolution line).  
- **CO5:** Fight with RNNs and survive LSTMs.  

---

### 📝 Units Breakdown (with sarcasm included)  

#### **UNIT – I**  
- Deep Learning intro (ML vs DL – the eternal “why so deep?” question).  
- ANN: From neurons to “Perceptrons” (a.k.a. dot product with attitude).  
- Fine-tuning hyperparameters (aka endless trial & error).  
- **Case Study:** Heart Disease Prediction (because why not start with saving lives?).  

#### **UNIT – II**  
- Deep Neural Networks – training them without exploding/vanishing gradients (spoiler: good luck).  
- Faster optimizers (SGD isn’t enough, now we bring Adam to the party).  
- Regularization: AKA “Don’t let your model cheat by memorizing.”  

#### **UNIT – III**  
- CNN Part 1: Convolutions, pooling, structured outputs.  
- Applications: Image recognition, object detection, and memes classification (probably).  

#### **UNIT – IV**  
- CNN Part 2: Architectures like LeNet5, AlexNet, GoogLeNet, ResNet.  
- Advantages of CNN (hint: they actually work).  
- **Case Study:** Handwritten digit recognition (MNIST – the “Hello World” of DL).  

#### **UNIT – V**  
- RNNs: From loops to memory cells (aka models that actually “remember”).  
- LSTM: Because vanilla RNNs forget faster than students after exams.  
- **Case Study:** Time series prediction (stocks, weather, or just your GPA).  

---

### 📖 Textbooks That Everyone Buys But Reads PDFs Instead  
1. *Hands-On Machine Learning with Scikit-Learn and TensorFlow* – Aurélien Géron (a.k.a. the only book you’ll actually read).  
2. *Deep Learning* – Ian Goodfellow, Yoshua Bengio, Aaron Courville (a.k.a. The Holy Bible of DL).  

---

### 📂 What’s in this Repo?  
- ✍️ My personal notes (written with love, sweat, and caffeine).  
- 💻 Code snippets for each unit (so you don’t cry while coding from scratch).  
- 🎓 Case studies implemented in Python/TensorFlow/Keras.  
- ⚡ Bonus: Extra examples, experiments, and probably some meme references.  

---

### 🚀 Why this repo?  
Because Deep Learning is *deep*… and so are the nightmares during exams.  
This repo = Notes + Codes = Survival kit for CS407.  

---

## 🌟 Contributions  
- If you’re in the same boat (a.k.a same course), feel free to add notes/code.  
- PRs welcome, but don’t send your entire syllabus PDF 😅.  

---

### 🏁 TL;DR  
This is my **Deep Learning (DLE) – VII Semester** repo with notes + code for each unit, case studies, and extra stuff.  
Use it wisely. Or just stare at the repo and pretend to study.  

